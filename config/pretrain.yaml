hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - hyperparams/tabpfn_finetune: tabpfn_finetune_10k

output_dir: ${hydra:run.dir}
seed: 0
devices: [0, 1, 2, 3]
workers_per_gpu: 4

model: null

optim:
  max_steps: 380
  log_every_n_steps: 10
  eval_every_n_steps: 100
  batch_size: 128     # total batch size (not per device)
  gradient_accumulation_steps: 1    # Accumulation steps are not counted towards max_steps
  lr: 1.e-4
  weight_decay: 0.000
  beta1: 0.9
  beta2: 0.999
  warmup_steps: 1_000
  cosine_scheduler: True  

data:
  min_samples: 1024
  max_samples: 1024      # Support + query
  min_features: 3
  max_features: 100
  max_classes: 10
  support_proportion: 0.8   # Proportion of support samples vs query samples

# Make loss plot
plotting:
  n_runs: 500                   # Number of runs to consider for plotting
  n_random_shuffles: 100         # Number of random shuffles for plotting the confidence boundaries
  confidence_bound: 0.90       # Confidence bounds ratio for plotting the confidence boundaries
  plot_default_value: True     # Plot the default value of the model

  benchmark_models:   # Based on which models do we normalize the accuracy and R2
    - MLP
    - RESNET
    - SAINT
    - FT_TRANSFORMER
    - RANDOM_FOREST
    - XGBOOST
    - GRADIENT_BOOSTING_TREE