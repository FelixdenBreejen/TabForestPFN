hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - hyperparams/tabpfn: finetune_10k
  - hyperparams/foundation: finetune_10k
  - pretrain_model: foundation

output_dir: ${hydra:run.dir}
seed: 0
devices: [0, 1, 2, 3, 4, 5, 6, 7]
workers_per_gpu: 4

optim:
  max_steps: 500_000                # Every step completes batch_size * gradient_accumulation_steps samples
  log_every_n_steps: 10
  eval_every_n_steps: 10_000
  batch_size: 64                    # Total batch size over all devices. 
  gradient_accumulation_steps: 1    # Accumulation steps are not counted towards max_steps
  lr: 1.e-4
  weight_decay: 0.00
  beta1: 0.9
  beta2: 0.999
  warmup_steps: 10_000
  cosine_scheduler: True  
  max_grad_norm: 0.1

data:
  min_samples_support: 128
  max_samples_support: 1024            
  n_samples_query: 256
  min_features: 3
  max_features: 100
  max_classes: 10


plotting:
  n_runs: 500                   # Number of runs to consider for plotting
  n_random_shuffles: 100         # Number of random shuffles for plotting the confidence boundaries
  confidence_bound: 0.90       # Confidence bounds ratio for plotting the confidence boundaries
  plot_default_value: True     # Plot the default value of the model

  benchmark_models:   # Based on which models do we normalize the accuracy and R2
    - MLP
    - RESNET
    - SAINT
    - FT_TRANSFORMER
    - RANDOM_FOREST
    - XGBOOST
    - GRADIENT_BOOSTING_TREE