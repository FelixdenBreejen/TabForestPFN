hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - _self_
  - hyperparams/tabpfn: finetune_10k
  - hyperparams/foundation: finetune_10k
  - pretrain_model: foundation
  # - pretrain_continue
  - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled  

output_dir: ${hydra:run.dir}
seed: 0
devices: [0, 1, 2, 3]
workers_per_gpu: 4

optim:
  max_steps: 500_000                # Every step completes batch_size * gradient_accumulation_steps samples
  warmup_steps: 10_000
  log_every_n_steps: 10
  eval_every_n_steps: 50
  batch_size: 64                     # Total batch size over all devices. 
  gradient_accumulation_steps: 1    # Accumulation steps are not counted towards max_steps
  lr: 1.e-4
  weight_decay: 0.00
  beta1: 0.9
  beta2: 0.95
  cosine_scheduler: True  
  max_grad_norm: 1.0
  use_pretrained_weights: False
  path_to_weights: outputs_done/foundation_key_att/weights/model_step_500000.pt               # Path to a checkpoint to load weights from

data:
  generator: forest             # tabpfn or forest
  min_samples_support: 128
  max_samples_support: 1024            
  n_samples_query: 256
  min_features: 3
  max_features: 100
  max_classes: 10

preprocessing:
  use_quantile_transformer: True         # TabPFN: True
  use_feature_count_scaling: False        # TabPFN: True


plotting:
  n_runs: 500                   # Number of runs to consider for plotting
  n_random_shuffles: 100         # Number of random shuffles for plotting the confidence boundaries
  confidence_bound: 0.90       # Confidence bounds ratio for plotting the confidence boundaries
  plot_default_value: True     # Plot the default value of the model

  benchmark_models:   # Based on which models do we normalize the accuracy and R2
    - MLP
    - RESNET
    - SAINT
    - FT_TRANSFORMER
    - RANDOM_FOREST
    - XGBOOST
    - GRADIENT_BOOSTING_TREE