hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}


defaults:
  - hyperparams/ft_transformer: ft_transformer
  - hyperparams/tabpfn_finetune: tabpfn_finetune
  - _self_


continue_last_output: False
output_dir: ${hydra:run.dir}
seed: 0
devices: [1, 3, 5, 6, 7]
runs_per_device: 1
runs_per_dataset: 1000                  # suggested: 500, only for random search
monitor_interval_in_seconds: 10       # plot the results every x seconds

models:
  - tabpfn_finetune
model_plot_names:
  - TabPFN Finetune
benchmarks:
  # - categorical_classification
  # - numerical_classification
  # - categorical_regression
  - numerical_regression
  # - categorical_classification_large
  # - numerical_classification_large
  # - categorical_regression_large
  # - numerical_regression_large
search_type:
  # - random
  - default

ignore_datasets:
  - 44140  # diamonds numerical regression
  # - 44142  # bike sharing numerical regression


plotting:
  n_runs: 500                   # Number of runs to consider for plotting
  n_random_shuffles: 100         # Number of random shuffles for plotting the confidence boundaries
  confidence_bound: 0.90       # Confidence bounds ratio for plotting the confidence boundaries

  benchmark_models:   # Based on which models do we normalize the accuracy and R2
    - MLP
    - Resnet
    - SAINT
    - FT Transformer
    - RandomForest
    - XGBoost
    - GradientBoostingTree